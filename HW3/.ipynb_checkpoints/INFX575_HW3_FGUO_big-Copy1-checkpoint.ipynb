{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import lda.datasets\n",
    "from __future__ import division, print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file locations\n",
    "DIR = '/Users/feismacbookpro/Desktop/INFX575/HW3/big/'\n",
    "\n",
    "# import groups.txt\n",
    "group = [line.strip() for line in open(DIR + 'groups2.txt')]\n",
    "group.pop(0)\n",
    "\n",
    "groups = {}\n",
    "for row in group:\n",
    "    row = row.split('\\t')\n",
    "    groups[row[0]] = row[1]\n",
    "# groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pID', 'abstract']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import abstracts and remove the rows with null\n",
    "abstract = [line.strip() for line in open(DIR + 'abstracts2.txt')]\n",
    "abstracts = []\n",
    "for row in abstract:\n",
    "    row = row.split('\\t')\n",
    "    if row[1] != 'null':\n",
    "        abstracts.append(row)\n",
    "abstracts.pop(0)\n",
    "# abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a corpus with all texts from all paragraphs\n",
    "text = []\n",
    "for row in abstracts:\n",
    "    text.append(row[1])\n",
    "text_all = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\n",
    "             \"before\",\"herself\",\"had\",\"should\",\"to\",\"only\",\"under\",\"ours\",\"has\",\n",
    "             \"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\"during\",\"now\",\"him\",\"nor\",\n",
    "             \"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\"few\",\"because\",\"doing\",\n",
    "             \"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\n",
    "             \"above\",\"between\",\"t\",\"be\",\"we\",\"who\",\"were\",\"here\",\"hers\",\"by\",\"on\",\n",
    "             \"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\"into\",\"yourself\",\"down\",\"your\",\n",
    "             \"from\",\"her\",\"their\",\"there\",\"been\",\"whom\",\"too\",\"themselves\",\"was\",\n",
    "             \"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\n",
    "             \"he\",\"me\",\"myself\",\"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\n",
    "             \"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\"at\",\"have\",\"in\",\"any\",\n",
    "             \"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\n",
    "             \"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\"the\",\"having\",\"once\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def rmStopWords(text_paragraph):\n",
    "    stopset = set(stopwords) #make stop words\n",
    "    exclude = set(string.punctuation) #save punctuations\n",
    "\n",
    "    #load text file and remove stop words\n",
    "    tokens = word_tokenize(str(text_paragraph).decode('utf-8'))\n",
    "    tokens = [w for w in tokens if not w in exclude]\n",
    "    tokens = [w.lower() for w in tokens if not w.lower() in stopset]\n",
    "    tokens = [w for w in tokens if len(w)>2]\n",
    "    tokens = list(tokens)\n",
    "    return tokens\n",
    "\n",
    "# remove stoprwords from all the paragraphs\n",
    "rm_stopWords = []\n",
    "for row in text:\n",
    "    rm_stopWords.append(rmStopWords(row))\n",
    "len(rm_stopWords)\n",
    "\n",
    "# rm_sw_textALL = rmStopWords(text_all)\n",
    "# len(rm_sw_textALL)\n",
    "#total words: 1686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine the pid with the paragraphs that have stop words removed in to a dictionary\n",
    "pid = []\n",
    "for i in range(len(abstracts)):\n",
    "    pid.append(abstracts[i][0])\n",
    "\n",
    "abstracts_new = {}\n",
    "for i in range(len(pid)):\n",
    "    abstracts_new[pid[i]] = rm_stopWords[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "\n",
    "# allocate paragraphs into groups by the groups dictionary\n",
    "group = defaultdict(list)\n",
    "for i in range(len(abstracts_new.keys())):\n",
    "    key = groups[abstracts_new.keys()[i]]\n",
    "    for word in abstracts_new.values()[i]:\n",
    "        group[key].append(unicodedata.normalize('NFKD', word).encode('ascii','ignore'))\n",
    "\n",
    "# remove group 0\n",
    "group.pop('0', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# for H_x for each group\n",
    "def shannon(group):\n",
    "    entrophy = []\n",
    "    count = Counter(group)\n",
    "    vocab = list(set(group))\n",
    "    H_x = 0\n",
    "    sum_count = sum(count.itervalues())\n",
    "    for word in vocab:\n",
    "        p_x = count[word]/sum_count\n",
    "        if p_x > 0:\n",
    "            H_x += - p_x * math.log(p_x, 2)\n",
    "    entrophy.append(H_x)\n",
    "    return entrophy\n",
    "\n",
    "# for Q(pi||pj)\n",
    "\n",
    "# function to calculate Q(pi||pj)\n",
    "def q_entropy(text_all, group1, group2):\n",
    "    count1 = Counter(group1)\n",
    "    vocab1 = list(set(group1))\n",
    "    count2 = Counter(group2)\n",
    "    vocab2 = list(set(group2))\n",
    "    s_count = Counter(text_all)\n",
    "    s_vocab = list(set(text_all))\n",
    "    sum1 = sum(count1.itervalues())\n",
    "    sum2 = sum(count2.itervalues())\n",
    "    s_sum = sum(s_count.itervalues())\n",
    "    q = 0\n",
    "    for w in vocab1:\n",
    "        p_1 = count1[w]/sum1\n",
    "        p_2 = count2[w]/sum2\n",
    "        s_x = s_count[w]/s_sum\n",
    "        p_s1 = 0.99 * p_1 + 0.01 * s_x\n",
    "        p_s2 = 0.99 * p_2 + 0.01 * s_x\n",
    "        q += - p_s1 * math.log(p_s2, 2)\n",
    "    return q\n",
    "\n",
    "# function to calculate the culture hole\n",
    "def JD(H1, group1, group2):\n",
    "#     H1 = shannon(group1) #calculate shannon entropy\n",
    "    text_all  = group1 + group2 #make a corpus which combines 2 groups\n",
    "    q_ij = q_entropy(text_all, group1, group2) #calculate q entropy\n",
    "    E_ij = H1[0] / q_ij\n",
    "    C_ij = 1 - E_ij\n",
    "    return format(C_ij, \".15g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         45778721 function calls in 25.284 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      100    4.495    0.045   24.162    0.242 <ipython-input-16-1ce746bfb33c>:21(q_entropy)\n",
      "      100    0.288    0.003   24.451    0.245 <ipython-input-16-1ce746bfb33c>:42(JD)\n",
      "       10    0.154    0.015    0.695    0.070 <ipython-input-16-1ce746bfb33c>:5(shannon)\n",
      "        1    0.137    0.137   25.284   25.284 <ipython-input-17-ff93b7eaf136>:1(jargan_distance)\n",
      "        1    0.000    0.000   25.284   25.284 <string>:1(<module>)\n",
      "      620    0.001    0.000    0.001    0.000 _weakrefset.py:70(__contains__)\n",
      "      310    0.002    0.000    0.003    0.000 abc.py:128(__instancecheck__)\n",
      "      310    0.003    0.000   19.756    0.064 collections.py:441(__init__)\n",
      "  1019870    0.093    0.000    0.093    0.000 collections.py:455(__missing__)\n",
      "      310   15.084    0.049   19.753    0.064 collections.py:504(update)\n",
      "      100    0.001    0.000    0.001    0.000 {format}\n",
      "      310    0.000    0.000    0.000    0.000 {getattr}\n",
      "      310    0.001    0.000    0.004    0.000 {isinstance}\n",
      "  1681526    0.263    0.000    0.263    0.000 {math.log}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      " 43074190    4.665    0.000    4.665    0.000 {method 'get' of 'dict' objects}\n",
      "      310    0.000    0.000    0.000    0.000 {method 'itervalues' of 'dict' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {range}\n",
      "      310    0.098    0.000    0.098    0.000 {sum}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def jargan_distance(group):\n",
    "    C = [[0 for x in range(10)] for x in range(10)] \n",
    "    for i in range(10):\n",
    "        H1 = shannon(group[str(i+1)])\n",
    "        for j in range(10):\n",
    "            C[i][j] = JD(H1, group[str(i+1)],group[str(j+1)])\n",
    "    return C\n",
    "\n",
    "cProfile.run( 'jargan_distance(group)' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
