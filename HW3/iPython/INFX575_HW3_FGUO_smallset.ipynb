{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import lda.datasets\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to load abstracts and cleaning them, make them into list of words,\n",
    "#and put them in groups according to the group file\n",
    "def load_data(DIR, groupfile, abstractfile):\n",
    "    # import groups.txt\n",
    "    group = [line.strip() for line in open(DIR + groupfile)]\n",
    "    group.pop(0)\n",
    "\n",
    "    groups = {} #make groups into a dictionary {pid:group#}\n",
    "    for row in group:\n",
    "        row = row.split('\\t')\n",
    "        groups[row[0]] = row[1] \n",
    "\n",
    "    # import abstracts and remove the rows with null, seperate key and value\n",
    "    abstract = [line.strip() for line in open(DIR + abstractfile)]\n",
    "    abstracts = []\n",
    "    for row in abstract:\n",
    "        row = row.split('\\t')\n",
    "        if row[1] != 'null':\n",
    "            abstracts.append(row)\n",
    "    abstracts.pop(0) #pop the column names\n",
    "\n",
    "    #make a new dictionary of abstracts stopwords removed then with pids as keys\n",
    "    abstracts_new = {}\n",
    "    for i in range(len(abstracts)):\n",
    "        abstracts_new[abstracts[i][0]] = rmStopWords(abstracts[i][1])\n",
    "\n",
    "    # allocate paragraphs into groups by the groups dictionary\n",
    "    group = defaultdict(list)\n",
    "    for i in range(len(abstracts_new.keys())):\n",
    "        key = groups[abstracts_new.keys()[i]]\n",
    "        for word in abstracts_new.values()[i]:\n",
    "            group[key].append(unicodedata.normalize('NFKD', word).encode('ascii','ignore'))\n",
    "\n",
    "    # remove column names for group\n",
    "    group.pop('0', None)\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to remove stop words from a paragraph is texts, return list of words\n",
    "def rmStopWords(text_paragraph):\n",
    "    stopwords = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\n",
    "             \"before\",\"herself\",\"had\",\"should\",\"to\",\"only\",\"under\",\"ours\",\"has\",\n",
    "             \"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\"during\",\"now\",\"him\",\"nor\",\n",
    "             \"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\"few\",\"because\",\"doing\",\n",
    "             \"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\n",
    "             \"above\",\"between\",\"t\",\"be\",\"we\",\"who\",\"were\",\"here\",\"hers\",\"by\",\"on\",\n",
    "             \"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\"into\",\"yourself\",\"down\",\"your\",\n",
    "             \"from\",\"her\",\"their\",\"there\",\"been\",\"whom\",\"too\",\"themselves\",\"was\",\n",
    "             \"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\n",
    "             \"he\",\"me\",\"myself\",\"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\n",
    "             \"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\"at\",\"have\",\"in\",\"any\",\n",
    "             \"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\n",
    "             \"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\"the\",\"having\",\"once\"]\n",
    "    \n",
    "    stopset = set(stopwords) #make stop words\n",
    "    exclude = set(string.punctuation) #save punctuations\n",
    "\n",
    "    #load text file and remove stop words\n",
    "    tokens = word_tokenize(str(text_paragraph).decode('utf-8'))\n",
    "    tokens = [w for w in tokens if not w in exclude]\n",
    "    tokens = [w.lower() for w in tokens if not w.lower() in stopset]\n",
    "    tokens = [w for w in tokens if len(w)>2]\n",
    "    tokens = list(tokens)\n",
    "    return tokens\n",
    "\n",
    "# for H_x for each group\n",
    "def shannon(group):\n",
    "    entrophy = []\n",
    "    count = Counter(group)\n",
    "    vocab = list(set(group))\n",
    "    H_x = 0\n",
    "    sum_count = sum(count.itervalues())\n",
    "    for word in vocab:\n",
    "        p_x = count[word]/sum_count\n",
    "        if p_x > 0:\n",
    "            H_x += - p_x * math.log(p_x, 2)\n",
    "    entrophy.append(H_x)\n",
    "    return entrophy\n",
    "\n",
    "# for Q(pi||pj)\n",
    "\n",
    "# function to calculate Q(pi||pj)\n",
    "def q_entropy(text_all, group1, group2):\n",
    "    count1 = Counter(group1) #word count for group1\n",
    "    vocab1 = list(set(group1)) #vocab for gourp1\n",
    "    \n",
    "    count2 = Counter(group2) #word count for group2\n",
    "    vocab2 = list(set(group2)) #vocab for gourp2\n",
    "    \n",
    "    s_count = Counter(text_all) #count for text_all\n",
    "    s_vocab = list(set(text_all)) #vocab for text_all\n",
    "    \n",
    "    sum1 = sum(count1.itervalues()) #sum of total counts for group1\n",
    "    sum2 = sum(count2.itervalues()) #sum of total counts for group2\n",
    "    s_sum = sum(s_count.itervalues()) #sum of total counts for text_all\n",
    "    \n",
    "    q = 0\n",
    "    for w in vocab1: #for each word in group1 vocab\n",
    "        p_1 = count1[w]/sum1\n",
    "        p_2 = count2[w]/sum2\n",
    "        s_x = s_count[w]/s_sum\n",
    "        p_s1 = 0.99 * p_1 + 0.01 * s_x\n",
    "        p_s2 = 0.99 * p_2 + 0.01 * s_x\n",
    "        q += - p_s1 * math.log(p_s2, 2)\n",
    "    return q\n",
    "\n",
    "# function to calculate the culture hole\n",
    "def JD(H1, group1, group2):\n",
    "    text_all  = group1 + group2 #make a corpus which combines 2 groups\n",
    "    q_ij = q_entropy(text_all, group1, group2) #calculate q entropy\n",
    "    E_ij = H1[0] / q_ij\n",
    "    C_ij = 1 - E_ij\n",
    "#     return format(C_ij, \".15g\")\n",
    "    return C_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jargan_distance(C, group, length):\n",
    "    for i in range(length):\n",
    "        H1 = shannon(group[str(i+1)])\n",
    "        for j in range(length):\n",
    "            C[i][j] = JD(H1, group[str(i+1)],group[str(j+1)])\n",
    "    return C\n",
    "\n",
    "def calculateJD(DIR, groupfile, abstractfile):\n",
    "    group = load_data(DIR, groupfile, abstractfile)\n",
    "    length = len(group)\n",
    "    C = [[0 for x in range(length)] for x in range(length)] \n",
    "    C = jargan_distance(C, group, length)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         68760483 function calls (68461293 primitive calls) in 54.950 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    1.351    1.351   29.513   29.513 <ipython-input-56-4bac46b42b65>:3(load_data)\n",
      "    10000    1.250    0.000   24.896    0.002 <ipython-input-57-de0e3abe7c7b>:2(rmStopWords)\n",
      "       10    0.147    0.015    0.659    0.066 <ipython-input-57-de0e3abe7c7b>:29(shannon)\n",
      "      100    4.585    0.046   24.263    0.243 <ipython-input-57-de0e3abe7c7b>:45(q_entropy)\n",
      "      100    0.297    0.003   24.560    0.246 <ipython-input-57-de0e3abe7c7b>:70(JD)\n",
      "        1    0.144    0.144   25.363   25.363 <ipython-input-58-1af9672a08c5>:1(jargan_distance)\n",
      "        1    0.055    0.055   54.931   54.931 <ipython-input-58-1af9672a08c5>:8(calculateJD)\n",
      "        1    0.019    0.019   54.950   54.950 <string>:1(<module>)\n",
      "    10000    0.020    0.000    6.503    0.001 __init__.py:75(sent_tokenize)\n",
      "    10000    0.188    0.000   23.133    0.002 __init__.py:86(word_tokenize)\n",
      "      620    0.001    0.000    0.001    0.000 _weakrefset.py:70(__contains__)\n",
      "      310    0.002    0.000    0.003    0.000 abc.py:128(__instancecheck__)\n",
      "      310    0.003    0.000   19.734    0.064 collections.py:441(__init__)\n",
      "  1019870    0.093    0.000    0.093    0.000 collections.py:455(__missing__)\n",
      "      310   14.908    0.048   19.731    0.064 collections.py:504(update)\n",
      "    10000    0.031    0.000    0.042    0.000 data.py:105(split_resource_url)\n",
      "    10000    0.058    0.000    0.351    0.000 data.py:131(normalize_resource_url)\n",
      "    10000    0.054    0.000    0.235    0.000 data.py:186(normalize_resource_name)\n",
      "    10000    0.038    0.000    0.404    0.000 data.py:693(load)\n",
      "    10000    0.039    0.000    0.064    0.000 posixpath.py:336(normpath)\n",
      "    10000    0.005    0.000    0.011    0.000 posixpath.py:59(isabs)\n",
      "    10000    0.016    0.000    6.079    0.001 punkt.py:1266(tokenize)\n",
      "    10000    0.054    0.000    6.025    0.001 punkt.py:1301(span_tokenize)\n",
      "    10000    0.038    0.000    6.063    0.001 punkt.py:1311(sentences_from_text)\n",
      "    84084    1.651    0.000    5.718    0.000 punkt.py:1320(_slices_from_text)\n",
      "    84057    0.162    0.000    5.972    0.000 punkt.py:1334(_realign_boundaries)\n",
      "    69669    0.165    0.000    3.949    0.000 punkt.py:1364(text_contains_sentbreak)\n",
      "    69669    0.039    0.000    0.039    0.000 punkt.py:1400(_annotate_tokens)\n",
      "   218883    0.210    0.000    3.741    0.000 punkt.py:1498(_annotate_second_pass)\n",
      "   149214    0.198    0.000    0.428    0.000 punkt.py:1508(_second_pass_annotation)\n",
      "     7736    0.033    0.000    0.047    0.000 punkt.py:1590(_ortho_heuristic)\n",
      "    69669    0.025    0.000    0.025    0.000 punkt.py:254(_word_tokenizer_re)\n",
      "    69669    0.061    0.000    0.455    0.000 punkt.py:270(word_tokenize)\n",
      "    10000    0.004    0.000    0.004    0.000 punkt.py:286(period_context_re)\n",
      "302967/84084    0.180    0.000    5.769    0.000 punkt.py:347(_pair_iter)\n",
      "   154983    0.514    0.000    1.197    0.000 punkt.py:432(__init__)\n",
      "   154983    0.117    0.000    0.395    0.000 punkt.py:455(_get_type)\n",
      "    71785    0.074    0.000    0.085    0.000 punkt.py:459(type_no_period)\n",
      "    77180    0.023    0.000    0.025    0.000 punkt.py:468(type_no_sentperiod)\n",
      "     9559    0.006    0.000    0.007    0.000 punkt.py:478(first_upper)\n",
      "     7301    0.004    0.000    0.004    0.000 punkt.py:483(first_lower)\n",
      "   151943    0.083    0.000    0.176    0.000 punkt.py:496(is_ellipsis)\n",
      "    69528    0.036    0.000    0.077    0.000 punkt.py:506(is_initial)\n",
      "   224652    0.414    0.000    2.141    0.000 punkt.py:577(_tokenize_words)\n",
      "   224652    0.161    0.000    2.935    0.000 punkt.py:604(_annotate_first_pass)\n",
      "   154983    0.380    0.000    0.637    0.000 punkt.py:625(_first_pass_annotation)\n",
      "    10000    0.010    0.000    0.044    0.000 re.py:143(search)\n",
      "  1120855    0.763    0.000    9.995    0.000 re.py:148(sub)\n",
      "  1130855    1.216    0.000    1.216    0.000 re.py:230(_compile)\n",
      "  1555197    0.701    0.000    1.070    0.000 re.py:264(_compile_repl)\n",
      "  1555197    1.094    0.000    2.164    0.000 re.py:284(_subx)\n",
      "   237130    0.153    0.000    0.784    0.000 re.py:290(filter)\n",
      "   237130    0.452    0.000    0.631    0.000 sre_parse.py:802(expand_template)\n",
      "    74057    0.919    0.000   16.442    0.000 treebank.py:59(tokenize)\n",
      "    10000    0.008    0.000    0.053    0.000 utf_8.py:15(decode)\n",
      "    10000    0.045    0.000    0.045    0.000 {_codecs.utf_8_decode}\n",
      "      310    0.000    0.000    0.000    0.000 {getattr}\n",
      "    10310    0.007    0.000    0.010    0.000 {isinstance}\n",
      "   149338    0.023    0.000    0.023    0.000 {iter}\n",
      "  1207629    0.081    0.000    0.081    0.000 {len}\n",
      "  1681526    0.265    0.000    0.265    0.000 {math.log}\n",
      "  1090601    0.088    0.000    0.088    0.000 {method 'append' of 'list' objects}\n",
      "    10000    0.014    0.000    0.068    0.000 {method 'decode' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "  1050590    0.657    0.000    0.657    0.000 {method 'encode' of 'unicode' objects}\n",
      "    65031    0.016    0.000    0.016    0.000 {method 'end' of '_sre.SRE_Match' objects}\n",
      "    10000    0.005    0.000    0.005    0.000 {method 'endswith' of 'str' objects}\n",
      "   223795    0.098    0.000    0.098    0.000 {method 'endswith' of 'unicode' objects}\n",
      "    69669    0.370    0.000    0.370    0.000 {method 'findall' of '_sre.SRE_Pattern' objects}\n",
      "    10000    0.013    0.000    0.013    0.000 {method 'finditer' of '_sre.SRE_Pattern' objects}\n",
      " 44649387    5.195    0.000    5.195    0.000 {method 'get' of 'dict' objects}\n",
      "   668192    0.127    0.000    0.127    0.000 {method 'group' of '_sre.SRE_Match' objects}\n",
      "     7301    0.001    0.000    0.001    0.000 {method 'islower' of 'unicode' objects}\n",
      "     9559    0.001    0.000    0.001    0.000 {method 'isupper' of 'unicode' objects}\n",
      "      310    0.000    0.000    0.000    0.000 {method 'itervalues' of 'dict' objects}\n",
      "    10000    0.004    0.000    0.004    0.000 {method 'join' of 'str' objects}\n",
      "   247130    0.127    0.000    0.127    0.000 {method 'join' of 'unicode' objects}\n",
      "    10001    1.100    0.000    1.100    0.000 {method 'keys' of 'dict' objects}\n",
      "  3152961    0.444    0.000    0.444    0.000 {method 'lower' of 'unicode' objects}\n",
      "   285555    0.175    0.000    0.175    0.000 {method 'match' of '_sre.SRE_Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "    10000    0.009    0.000    0.009    0.000 {method 'replace' of 'str' objects}\n",
      "    10000    0.006    0.000    0.006    0.000 {method 'replace' of 'unicode' objects}\n",
      "      434    0.000    0.000    0.000    0.000 {method 'rstrip' of 'unicode' objects}\n",
      "    10000    0.017    0.000    0.017    0.000 {method 'search' of '_sre.SRE_Pattern' objects}\n",
      "    40001    0.056    0.000    0.056    0.000 {method 'split' of 'str' objects}\n",
      "   217461    0.360    0.000    0.360    0.000 {method 'split' of 'unicode' objects}\n",
      "    63571    0.014    0.000    0.014    0.000 {method 'start' of '_sre.SRE_Match' objects}\n",
      "    40000    0.020    0.000    0.020    0.000 {method 'startswith' of 'str' objects}\n",
      "    20002    0.007    0.000    0.007    0.000 {method 'strip' of 'str' objects}\n",
      "    69669    0.010    0.000    0.010    0.000 {method 'strip' of 'unicode' objects}\n",
      "  2016408   10.610    0.000   13.558    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
      "    10000    1.028    0.000    1.028    0.000 {method 'values' of 'dict' objects}\n",
      "149338/69031    0.039    0.000    2.534    0.000 {next}\n",
      "        2    0.000    0.000    0.000    0.000 {open}\n",
      "       24    0.000    0.000    0.000    0.000 {range}\n",
      "   914253    0.219    0.000    0.219    0.000 {setattr}\n",
      "      310    0.097    0.000    0.097    0.000 {sum}\n",
      "  1050590    0.350    0.000    0.350    0.000 {unicodedata.normalize}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#results for the large set\n",
    "from matplotlib.pyplot import show\n",
    "import cProfile\n",
    "\n",
    "#file locations\n",
    "DIR = '/Users/feismacbookpro/Desktop/INFX575/HW3/big/'\n",
    "groupfile = 'groups2.txt'\n",
    "abstractfile = 'abstracts2.txt'\n",
    "\n",
    "import cProfile\n",
    "cProfile.run( 'calculateJD(DIR, groupfile, abstractfile)' )\n",
    "\n",
    "# C = calculateJD(DIR, groupfile, abstractfile)\n",
    "\n",
    "# Z = linkage(C)\n",
    "# dendo = dendrogram(Z)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('/Users/feismacbookpro/Desktop/INFX575/HW3/big/dendo.jason', 'w') as f:\n",
    "#     json.dump(dendo, f)\n",
    "# # show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1885 function calls in 0.004 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.003    0.003 <ipython-input-56-4bac46b42b65>:3(load_data)\n",
      "        7    0.000    0.000    0.003    0.000 <ipython-input-57-de0e3abe7c7b>:2(rmStopWords)\n",
      "        3    0.000    0.000    0.000    0.000 <ipython-input-57-de0e3abe7c7b>:29(shannon)\n",
      "        9    0.000    0.000    0.000    0.000 <ipython-input-57-de0e3abe7c7b>:45(q_entropy)\n",
      "        9    0.000    0.000    0.001    0.000 <ipython-input-57-de0e3abe7c7b>:70(JD)\n",
      "        1    0.000    0.000    0.001    0.001 <ipython-input-58-1af9672a08c5>:1(jargan_distance)\n",
      "        1    0.000    0.000    0.004    0.004 <ipython-input-58-1af9672a08c5>:8(calculateJD)\n",
      "        1    0.000    0.000    0.004    0.004 <string>:1(<module>)\n",
      "        7    0.000    0.000    0.001    0.000 __init__.py:75(sent_tokenize)\n",
      "        7    0.000    0.000    0.003    0.000 __init__.py:86(word_tokenize)\n",
      "       60    0.000    0.000    0.000    0.000 _weakrefset.py:70(__contains__)\n",
      "       30    0.000    0.000    0.000    0.000 abc.py:128(__instancecheck__)\n",
      "       30    0.000    0.000    0.000    0.000 collections.py:441(__init__)\n",
      "       18    0.000    0.000    0.000    0.000 collections.py:455(__missing__)\n",
      "       30    0.000    0.000    0.000    0.000 collections.py:504(update)\n",
      "        7    0.000    0.000    0.000    0.000 data.py:105(split_resource_url)\n",
      "        7    0.000    0.000    0.000    0.000 data.py:131(normalize_resource_url)\n",
      "        7    0.000    0.000    0.000    0.000 data.py:186(normalize_resource_name)\n",
      "        7    0.000    0.000    0.000    0.000 data.py:693(load)\n",
      "        7    0.000    0.000    0.000    0.000 posixpath.py:336(normpath)\n",
      "        7    0.000    0.000    0.000    0.000 posixpath.py:59(isabs)\n",
      "        7    0.000    0.000    0.000    0.000 punkt.py:1266(tokenize)\n",
      "        7    0.000    0.000    0.000    0.000 punkt.py:1301(span_tokenize)\n",
      "        7    0.000    0.000    0.000    0.000 punkt.py:1311(sentences_from_text)\n",
      "       14    0.000    0.000    0.000    0.000 punkt.py:1320(_slices_from_text)\n",
      "       14    0.000    0.000    0.000    0.000 punkt.py:1334(_realign_boundaries)\n",
      "        7    0.000    0.000    0.000    0.000 punkt.py:286(period_context_re)\n",
      "       14    0.000    0.000    0.000    0.000 punkt.py:347(_pair_iter)\n",
      "        7    0.000    0.000    0.000    0.000 re.py:143(search)\n",
      "      112    0.000    0.000    0.001    0.000 re.py:148(sub)\n",
      "      119    0.000    0.000    0.000    0.000 re.py:230(_compile)\n",
      "      147    0.000    0.000    0.000    0.000 re.py:264(_compile_repl)\n",
      "      147    0.000    0.000    0.000    0.000 re.py:284(_subx)\n",
      "        7    0.000    0.000    0.002    0.000 treebank.py:59(tokenize)\n",
      "        7    0.000    0.000    0.000    0.000 utf_8.py:15(decode)\n",
      "        7    0.000    0.000    0.000    0.000 {_codecs.utf_8_decode}\n",
      "       30    0.000    0.000    0.000    0.000 {getattr}\n",
      "       37    0.000    0.000    0.000    0.000 {isinstance}\n",
      "        7    0.000    0.000    0.000    0.000 {iter}\n",
      "       24    0.000    0.000    0.000    0.000 {len}\n",
      "       40    0.000    0.000    0.000    0.000 {math.log}\n",
      "       46    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'decode' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'encode' of 'unicode' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'finditer' of '_sre.SRE_Pattern' objects}\n",
      "      343    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "       30    0.000    0.000    0.000    0.000 {method 'itervalues' of 'dict' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'join' of 'unicode' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "       28    0.000    0.000    0.000    0.000 {method 'lower' of 'unicode' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'replace' of 'unicode' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'search' of '_sre.SRE_Pattern' objects}\n",
      "       29    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'split' of 'unicode' objects}\n",
      "       28    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "       16    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "      182    0.001    0.000    0.001    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {next}\n",
      "        2    0.000    0.000    0.000    0.000 {open}\n",
      "       10    0.000    0.000    0.000    0.000 {range}\n",
      "       30    0.000    0.000    0.000    0.000 {sum}\n",
      "       14    0.000    0.000    0.000    0.000 {unicodedata.normalize}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.9182958340544893, 7.942083298916778, 9.261921147523438],\n",
       " [7.576721142266502, 1.5, 9.098136908825852],\n",
       " [9.408989578954115, 9.09813690882585, 1.5]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results for the small test set \n",
    "\n",
    "#file locations\n",
    "DIR = '/Users/feismacbookpro/Desktop/INFX575/HW3/final/'\n",
    "groupfile = 'group1.txt'\n",
    "abstractfile = 'abstract1.txt'\n",
    "\n",
    "import cProfile\n",
    "cProfile.run( 'calculateJD(DIR, groupfile, abstractfile)' )\n",
    "\n",
    "C_final = calculateJD(DIR, groupfile, abstractfile)\n",
    "# C_final\n",
    "\n",
    "group = load_data(DIR, groupfile, abstractfile)\n",
    "group\n",
    "\n",
    "#calculating the q for trouble shooting\n",
    "length = len(group)\n",
    "H1 = []\n",
    "Q = [[0 for x in range(length)] for x in range(length)]\n",
    "for i in range(length):\n",
    "#     H1.append(shannon(group[str(i+1)]))\n",
    "    for j in range(length):\n",
    "        group1 = group[str(i+1)]\n",
    "        group2 = group[str(j+1)] \n",
    "        text_all  = group1 + group2\n",
    "        Q[i][j] = q_entropy(text_all, group1, group2)\n",
    "#             C[i][j] = JD(H1, group[str(i+1)],group[str(j+1)])\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  1.10467076,  2.        ],\n",
       "       [ 2.        ,  3.        ,  1.15806456,  3.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "Z = linkage(C_final)\n",
    "# dendrogram(Z)\n",
    "Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
